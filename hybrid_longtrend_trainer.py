# trainers/hybrid_longtrend_trainer.py
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim

import gc, optuna, psutil, math
import numpy as np
import pandas as pd

from torch.utils.data import Dataset, DataLoader
from sklearn.ensemble import RandomForestClassifier
import lightgbm as lgb
import xgboost  as xgb
from sklearn.metrics import log_loss
from sklearn.model_selection import TimeSeriesSplit
from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback
# ganz oben in hybrid_longtrend_trainer.py âœ Importâ€‘Block ergÃ¤nzen
from transformers.trainer_utils import IntervalStrategy

from rtdl import FTTransformer
from peft import LoraConfig, get_peft_model

from utils.dataset   import LongTrendDataset
from utils.collate   import numeric_collate, meta_collate
from utils.features  import enrich
from .meta_transformer import MetaTransformer
from .base import BaseTrainer
# ganz oben einmalig ergÃ¤nzen (falls noch nicht vorhanden):
from transformers import EarlyStoppingCallback, IntervalStrategy
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helperâ€‘datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class NumpyDataset(Dataset):
    def __init__(self, X, y):
        self.X, self.y = X, y
    def __len__(self):          return len(self.y)
    def __getitem__(self, idx): return {"x_num": self.X[idx], "label": self.y[idx]}

# â”€â”€â”€â”€â”€ Focalâ€‘Loss (binary) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class FocalLoss(nn.Module):
    def __init__(self, gamma: float = 2.0, pos_weight: torch.Tensor | None = None):
        super().__init__()
        self.gamma = gamma
        self.bce   = nn.BCEWithLogitsLoss(reduction="none", pos_weight=pos_weight)

    def forward(self, logits, targets):
        bce  = self.bce(logits, targets)
        prob = torch.sigmoid(logits)
        pt   = torch.where(targets == 1, prob, 1 - prob)        # p_t
        focal = (1 - pt) ** self.gamma * bce
        return focal.mean()

class FTWrapped(nn.Module):
    """
    Binaryâ€‘Wrapper
      â€¢ BCEWithLogits + optionale Classâ€‘Imbalanceâ€‘Gewichte
      â€¢ Labelâ€‘Smoothing  (eps)
      â€¢ Focalâ€‘Lossâ€‘Faktor (gamma)  â€“â€¯wenn gamma > 0
    """
    def __init__(
        self,
        ft_base: nn.Module,
        pos_weight: torch.Tensor | None = None,
        label_smooth_eps: float = 0.0,
        focal_gamma: float | None = None,          # â† NEU
    ):
        super().__init__()
        self.ft   = ft_base
        self.eps  = label_smooth_eps
        self.gamma= focal_gamma
        self.bce  = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction="none")

    def forward(self, x_num, labels: torch.Tensor | None = None):
        logits = self.ft(x_num, None).squeeze(-1)        # [B]
        loss   = None

        if labels is not None:
            # â”€â”€ Labelâ€‘Smoothing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if self.eps > 0.0:
                labels = labels * (1.0 - self.eps) + 0.5 * self.eps

            bce_loss = self.bce(logits, labels)

            # â”€â”€ Focalâ€‘Modulation (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if self.gamma and self.gamma > 0:
                p_t = torch.sigmoid(logits).detach()
                focal_factor = (1.0 - p_t).pow(self.gamma)
                bce_loss = focal_factor * bce_loss

            loss = bce_loss.mean()

        return {"logits": logits, "loss": loss}


class SimpleCNN(nn.Module):
    def __init__(self, n_feat: int):
        super().__init__()
        # EingabeÂ­form:  x  = [B, n_feat, seq_len]
        self.net = nn.Sequential(
            nn.Conv1d(n_feat, 32, kernel_size=3, padding=1),  # 32 Filter
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),                          # -> [B,32,1]
            nn.Flatten(),                                     # -> [B,32]
            nn.Linear(32, 1)                                  # -> [B,1]
        )

    def forward(self, x):                                     # x: [B, n_feat, L]
        return self.net(x).squeeze(-1)                        # -> [B]


class MetaDataset(Dataset):
    def __init__(self, preds, y): self.preds, self.y = preds, y
    def __len__(self): return len(self.y)
    def __getitem__(self,i): return {"preds":self.preds[i],"labels":self.y[i]}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Trainerâ€‘Klasse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HybridLongTrendTrainer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class HybridLongTrendTrainer(BaseTrainer):
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  Daten laden & Valâ€‘Split  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Daten laden  (ohne Trialâ€‘AbhÃ¤ngigkeit)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Daten laden â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def load_data(self):
        num_cols = [
            "Open", "high", "low", "Close", "Volume",
            "sma_10", "ema_20", "rsi_14", "hour_sin", "hour_cos"
        ]
        # feste LÃ¤nge aus der YAML
        seq_len = self.cfg["training"].get("seq_len", 24)

        ds = LongTrendDataset(
            csv_path       = f"{self.cfg['data']['raw_dir']}/{self.cfg['data']['longtrend_file']}",
            numerical_cols = num_cols,
            seq_len        = seq_len
        )

        # â”€â”€ TimeSeriesSplit mit grÃ¶ÃŸerem Gap + 5â€¯% Positives â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        def last_fold_with_enough_pos(X, y, n_splits=5, min_frac=0.05):
            tss   = TimeSeriesSplit(n_splits=n_splits, gap=2*seq_len)
            folds = list(tss.split(X))
            for tr, va in reversed(folds):
                if y[va].mean() >= min_frac:
                    return tr, va
            return folds[-1]

        tr_idx, va_idx = last_fold_with_enough_pos(
            ds.X_seq, ds.y_seq, n_splits=5, min_frac=0.05
        )
        self.X_train, self.y_train = ds.X_seq[tr_idx], ds.y_seq[tr_idx]
        self.X_val,   self.y_val   = ds.X_seq[va_idx], ds.y_seq[va_idx]

        return self.X_train, self.y_train

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Utility 3â€‘D â†’ 2â€‘D â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    @staticmethod
    def _flat(X3d: np.ndarray) -> np.ndarray:
        return X3d.reshape(len(X3d), -1)

    def build_features(self, X):  return X               # Identityâ€‘Hook
    def train_final(self,*_,**__): return self.meta      # liefert Metaâ€‘Modell

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Miniâ€‘Batchâ€‘LogLoss (smoothing aware) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def _batch_log_loss(model, X, y, batch_size: int = 1024) -> float:
        """Valâ€‘Loss identisch zur Trainingsâ€‘Loss, auch wenn FTWrapped geÃ¤ndert wurde."""
        model.eval()
        loss_list = []

        # â‡£ passende Lossâ€‘Funktion finden oder rekonstruieren
        if hasattr(model, "bce"):
            base_loss = model.bce          # neue Namensvariante
            eps       = getattr(model, "eps", 0.0)
        elif hasattr(model, "crit"):
            base_loss = model.crit         # alte Variante
            eps       = getattr(model, "eps", 0.0)
        else:
            # Fallback â€“ einfaches BCE (ohne focal)
            pos_weight = torch.tensor([(len(y) - y.sum()) / (y.sum() + 1e-6)])
            base_loss  = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction="none")
            eps        = 0.0

        with torch.no_grad():
            for i in range(0, len(y), batch_size):
                xb = torch.tensor(X[i:i+batch_size], dtype=torch.float32)
                lb = torch.tensor(y[i:i+batch_size], dtype=torch.float32)

                if eps > 0:
                    lb = lb * (1.0 - eps) + 0.5 * eps

                logits = model(xb)["logits"]
                loss   = base_loss(logits, lb).mean()
                loss_list.append(loss.cpu())

        return torch.stack(loss_list).mean().item()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Basisâ€‘Modelle â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def _train_rf(self, X, y):
        Xf, rng, models = self._flat(X), np.random.default_rng(42), []
        for _ in range(3):
            idx = rng.choice(len(Xf), len(Xf), replace=True)
            m   = RandomForestClassifier(**self.cfg["model"]["rf_params"])
            m.fit(Xf[idx], y[idx]); models.append(m)
        return models

    def _train_lgb(self, X, y):
        Xf, rng, models = self._flat(X), np.random.default_rng(0), []
        for _ in range(3):
            idx = rng.choice(len(Xf), len(Xf), replace=True)
            d   = lgb.Dataset(Xf[idx], label=y[idx])
            m   = lgb.train(dict(objective="binary", learning_rate=0.05,
                                 num_leaves=64, metric="binary_logloss"),
                            d, 300)
            models.append(m)
        return models

    def _train_xgb(self, X, y):
        Xf, rng, models = self._flat(X), np.random.default_rng(1), []
        for _ in range(3):
            idx = rng.choice(len(Xf), len(Xf), replace=True)
            d   = xgb.DMatrix(Xf[idx], label=y[idx])
            m   = xgb.train(dict(objective="binary:logistic", eta=0.05,
                                 max_depth=6, eval_metric="logloss"),
                            d, 300)
            models.append(m)
        return models

    # -------- 1â€‘Dâ€‘CNN Basismodell --------
    def _train_cnn(self, X: np.ndarray, y: np.ndarray):
        """
        X: ndarray  [N, seq_len, n_feat]
        y: ndarray  [N]
        """
        mdl = SimpleCNN(n_feat=X.shape[2]).to(device)

        # â†’  Torch-Tensors auf den richtigen Device
        X_t = torch.tensor(X, dtype=torch.float32, device=device).permute(0, 2, 1)  # [N, n_feat, L]
        y_t = torch.tensor(y, dtype=torch.float32, device=device).view(-1)          # [N]  (â€¼ï¸ SQUEEZE)

        crit = nn.BCEWithLogitsLoss()
        opt  = optim.Adam(mdl.parameters(), lr=1e-3)

        mdl.train()
        for _ in range(10):
            opt.zero_grad()
            logits = mdl(X_t)                 # [N]
            loss   = crit(logits, y_t)
            loss.backward()
            opt.step()

        #  fÃ¼r spÃ¤tere CPU-Inference wieder zurÃ¼ck-schieben
        return mdl.cpu()
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # RAMâ€‘schonende Inferenz fÃ¼r FT
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _predict_ft_logits(self, X_num: np.ndarray, batch_size: int = 1024) -> np.ndarray:
        """
        Berechnet Logits des FTâ€‘Transformers stÃ¼ckweise, um Speicher zu sparen.

        Parameters
        ----------
        X_num : ndarray  [N, 240]  flach â‡’ 24â€¯Ã—â€¯10 numerische Features
        batch_size : int  GrÃ¶ÃŸe der Miniâ€‘Batches fÃ¼r die Inferenz
        """
        self.ft.eval()                          # kein Dropout
        preds = []

        with torch.no_grad():                   # Autograd ausschalten
            for i in range(0, len(X_num), batch_size):
                x_batch = torch.tensor(
                    X_num[i : i + batch_size],
                    dtype=torch.float32,
                    device=self.device         # self.device ist cpu | cuda
                )
                logits = self.ft(x_batch, None).squeeze(-1)  # [B]
                preds.append(logits.cpu().numpy())

        return np.concatenate(preds, axis=0)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• FTâ€‘Transformer Helfer â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  FTâ€‘Transformer Fabrik  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def _make_ft(self, n_features: int, n_blocks: int) -> FTTransformer:
        """
        Erstellt einen FTâ€‘Transformer und setzt anschlieÃŸend hÃ¶here Dropoutâ€‘
        Raten auf Attentionâ€‘ sowie FFNâ€‘Ebene (0.2).  Das geht, weil
        rtdl.FTTransformer.make_default die Layer als Dict speichert.
        """
        ft = FTTransformer.make_default(       # ohne Dropoutâ€‘Args
            n_num_features    = n_features,
            cat_cardinalities = (),
            d_out             = 1,
            n_blocks          = n_blocks
        )

        # â”€â”€  Attention & FFNâ€‘Dropout pro Block anpassen  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for blk in ft.transformer.blocks:               # type: ignore[attr-defined]
            blk['attention'].dropout.p = 0.2            # Attentionâ€‘Dropout
            blk['ffn'].dropout.p       = 0.2            # FFNâ€‘Dropout

        return ft

    def _add_lora(self, ft, n_blocks, rank=4):
        last2 = range(n_blocks-2, n_blocks)
        target = [f"blocks.{i}.{p}"
                  for i in last2
                  for p in ["attention.W_q","attention.W_k","attention.W_v",
                            "attention.W_out","ffn.linear_first",
                            "ffn.linear_second"]]
        return get_peft_model(
            ft,
            LoraConfig(r=rank,
                       lora_alpha=8*rank,
                       lora_dropout=0.05,
                       target_modules=target)
        )

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FT + Optuna Objective â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FTâ€‘Optunaâ€‘Objective â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  FTâ€‘Optunaâ€‘Objective  (inkl. seq_lenâ€‘Search, Earlyâ€‘Stopping, Focal Loss)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Optunaâ€‘Objective â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _ft_objective(self, trial, Xf, y):
        hp = {
            "n_blocks": trial.suggest_int("n_blocks", 2, 6),
            "lr"      : trial.suggest_float("lr", 1e-5, 3e-4, log=True),
        }

        pos_weight = torch.tensor([(len(y) - y.sum()) / (y.sum() + 1e-6)])
        ft   = self._make_ft(Xf.shape[1], hp["n_blocks"])

        # kleiner LoRAâ€‘Rank
        last2  = range(hp["n_blocks"] - 2, hp["n_blocks"])
        target = [f"blocks.{i}.{p}"
                for i in last2
                for p in ["attention.W_q", "attention.W_k", "attention.W_v",
                            "attention.W_out", "ffn.linear_first",
                            "ffn.linear_second"]]
        ft = get_peft_model(ft, LoraConfig(r=4, lora_alpha=16,
                                       lora_dropout=0.05,
                                       target_modules=target))

        model = FTWrapped(ft,
                        pos_weight    = pos_weight,
                        label_smooth_eps = 0.10,
                        focal_gamma      = 2.0)

        split = int(0.8 * len(y))
        ds_train = NumpyDataset(Xf[:split],  y[:split])
        ds_val   = NumpyDataset(Xf[split:], y[split:])

        args = TrainingArguments(
            output_dir                  = "opt_ft_tmp",
            per_device_train_batch_size = 32,
            num_train_epochs            = 6,
            learning_rate               = hp["lr"],
            weight_decay                = 1e-2,
            fp16                        = True,
            logging_steps               = 50,
            report_to                   = [],      # keine externen Logger
        )
        # â–º Earlyâ€‘Stoppingâ€‘Settings  (neu: eval_strategy statt evaluation_strategy)
        args.metric_for_best_model  = "eval_loss"
        args.load_best_model_at_end = True
        args.eval_strategy          = IntervalStrategy.EPOCH  

        trainer = Trainer(
            model         = model,
            args          = args,
            train_dataset = ds_train,
            eval_dataset  = ds_val,
            data_collator = numeric_collate,
        )
        from transformers import EarlyStoppingCallback
        trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))
        trainer.train()

        val_loss = self._batch_log_loss(model, Xf[split:], y[split:])

        del trainer, model, ft, ds_train, ds_val
        torch.cuda.empty_cache(); gc.collect()
        return val_loss

    # AufrÃ¤umâ€‘Callback nach jedem Trial
    def _cleanup_callback(self, study, trial):
        import shutil, glob
        for ckpt in glob.glob("opt_ft_tmp/checkpoint-*"):
            shutil.rmtree(ckpt, ignore_errors=True)
        torch.cuda.empty_cache(); gc.collect()


    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FTâ€‘Training (Optuna + Finalâ€‘Fit) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _train_ft(self, X, y):  
        Xf = self._flat(X)                              # (N, L*F)

        # ---------- Optunaâ€‘Suche -------------------------------------------------
        ft_study = optuna.create_study(direction="minimize")
        ft_study.optimize(
            lambda t: self._ft_objective(t, Xf, y),
            n_trials          = self.cfg["optuna"]["n_trials"],
            callbacks         = [self._cleanup_callback],
            show_progress_bar = False,
        )

        best = ft_study.best_params
        print("ğŸŸ¢  FTâ€‘best:", best)

        # ---------- Finalâ€‘Fit mit besten Parametern -----------------------------
        pos_weight = torch.tensor([(len(y) - y.sum()) / (y.sum() + 1e-6)])

        ft = self._make_ft(Xf.shape[1], best["n_blocks"])

        last2  = range(best["n_blocks"] - 2, best["n_blocks"])
        target = [f"blocks.{i}.{p}"
                for i in last2
                for p in ["attention.W_q", "attention.W_k", "attention.W_v",
                            "attention.W_out", "ffn.linear_first",
                            "ffn.linear_second"]]
        ft = get_peft_model(ft, LoraConfig(r=4, lora_alpha=16,
                                        lora_dropout=0.05,
                                        target_modules=target))

        model = FTWrapped(ft,
                        pos_weight       = pos_weight,
                        label_smooth_eps = 0.10,
                        focal_gamma      = 2.0)

        # â”€â”€ Optional: SSLâ€‘Weights vor dem Fineâ€‘Tuning laden â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        from pathlib import Path

        if getattr(self, "ssl_weights", None) and Path(self.ssl_weights).exists():
            print(f"â–¶  Lade SSL-Weights von {self.ssl_weights}")
            # model.ft ist dein PEFTâ€‘Model, strict=False ignoriert fehlende Keys
            model.ft.load_state_dict(torch.load(self.ssl_weights), strict=False)

        final_ds   = NumpyDataset(Xf, y)

        final_args = TrainingArguments(
            output_dir                  = "ft_final",
            per_device_train_batch_size = 32,
            num_train_epochs            = 8,
            learning_rate               = best["lr"],
            weight_decay                = 1e-2,
            fp16                        = True,
            logging_steps               = 50,
            report_to                   = [],
        )

        Trainer(model=model, args=final_args,
                train_dataset=final_ds,
                data_collator=numeric_collate).train()

        torch.cuda.empty_cache(); gc.collect()
        return model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  Metaâ€‘Stacking / Optimieren  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def optimize(self, X, y):
        X_flat      = self._flat(X)

        # â‘  Basismodelle trainieren
        self.rf_list  = self._train_rf(X, y)
        self.ft       = self._train_ft(X, y)           # â† nutzt intern GPU Ã¼ber HF-Trainer
        self.lgb_list = self._train_lgb(X, y)
        self.xgb_list = self._train_xgb(X, y)
        self.cnn      = self._train_cnn(X, y)

        # â‘¡ Vorhersagen **fÃ¼r den Val-Fold** erzeugen
        X_val_flat = self._flat(self.X_val)

        def mean_preds(models, Xtab, kind):
            if kind == "rf":
                return np.mean([m.predict_proba(Xtab)[:, 1] for m in models], axis=0)
            if kind == "lgb":
                return np.mean([m.predict(Xtab) for m in models], axis=0)
            if kind == "xgb":
                return np.mean([m.predict(xgb.DMatrix(Xtab)) for m in models], axis=0)

        preds_val = np.stack([
            mean_preds(self.rf_list,  X_val_flat, "rf"),
            mean_preds(self.lgb_list, X_val_flat, "lgb"),
            mean_preds(self.xgb_list, X_val_flat, "xgb"),
            # â”€ FT (gestÃ¼ckelt, spart RAM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            torch.sigmoid(
                torch.tensor(
                    self._predict_ft_logits(X_val_flat),
                    dtype=torch.float32
                )
            ).numpy(),
            # CNN â€“ permutieren bleibt gleich
            torch.sigmoid(
                self.cnn(
                    torch.tensor(self.X_val, dtype=torch.float32, device=self.device).permute(0, 2, 1)
                )
            ).detach().cpu().numpy()
        ], axis=1).astype(np.float32)

        # â‘¢ Metaâ€‘Modell NUR auf Valâ€‘Fold trainieren (reduziert Leakage / Overfitting)
        meta_ds   = MetaDataset(preds_val, self.y_val)
        meta_args = TrainingArguments(
            output_dir                  = "meta_runs",
            per_device_train_batch_size = 128,
            num_train_epochs            = 10,
            learning_rate               = 1e-3,
            fp16                        = torch.cuda.is_available(),
            logging_steps               = 50,
            report_to                   = []   # â† FIX: '=' eingefÃ¼gt
        )


        self.meta = MetaTransformer(self.cfg, n_models=preds_val.shape[1]).to(device)
        Trainer(model=self.meta,
                args=meta_args,
                train_dataset=meta_ds,
                data_collator=meta_collate).train()

        class Dummy: best_params = {}
        return Dummy()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• Speichern â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def save_model(self, _):
        self.model_dir.mkdir(parents=True, exist_ok=True)
        torch.save(self.ft.ft.state_dict(), self.model_dir / "ft.pt")
        torch.save(self.ft.state_dict(),    self.model_dir / "ft_wrap.pt")
        torch.save(self.cnn.state_dict(),   self.model_dir / "cnn.pt")
        torch.save(self.meta.state_dict(),  self.model_dir / "meta.pt")

        import joblib
        joblib.dump(self.rf_list,  self.model_dir / "rf_list.pkl")
        joblib.dump(self.lgb_list, self.model_dir / "lgb_list.pkl")
        joblib.dump(self.xgb_list, self.model_dir / "xgb_list.pkl")
        print(f"âœ…  Modelle gespeichert in {self.model_dir}")